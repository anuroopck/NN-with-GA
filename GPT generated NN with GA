import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def forward_propagation(X, weights, biases, activation_functions):
    """
    Performs forward propagation through an MLP with predefined weights and biases.
    :param X: Input array (n_samples, n_features)
    :param weights: List of weight matrices for each layer
    :param biases: List of bias vectors for each layer
    :param activation_functions: List of activation functions for each layer
    :return: Output after forward pass
    """
    A = X
    for W, b, activation in zip(weights, biases, activation_functions):
        Z = np.dot(A, W) + b  # Linear transformation
        A = activation(Z)  # Apply activation function
    return A

def initialize_population(pop_size, layers):
    """Initialize a population of weight matrices and biases."""
    population = []
    for _ in range(pop_size):
        individual = {
            "weights": [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)],
            "biases": [np.random.randn(layers[i+1]) for i in range(len(layers)-1)]
        }
        population.append(individual)
    return population

def fitness_function(individual, X, y):
    """Calculate fitness as inverse error."""
    output = forward_propagation(X, individual["weights"], individual["biases"], [relu, sigmoid])
    error = np.mean((output - y) ** 2)  # Mean Squared Error
    return 1 / (error + 1e-6)  # Avoid division by zero

def select_parents(population, fitnesses, num_parents):
    """Select best individuals based on fitness."""
    sorted_indices = np.argsort(fitnesses)[::-1]
    return [population[i] for i in sorted_indices[:num_parents]]

def crossover(parent1, parent2):
    """Perform crossover between two parents."""
    child = {"weights": [], "biases": []}
    for w1, w2, b1, b2 in zip(parent1["weights"], parent2["weights"], parent1["biases"], parent2["biases"]):
        w_child = (w1 + w2) / 2  # Average crossover
        b_child = (b1 + b2) / 2
        child["weights"].append(w_child)
        child["biases"].append(b_child)
    return child

def mutate(individual, mutation_rate=0.1):
    """Apply random mutation to weights and biases."""
    for i in range(len(individual["weights"])):
        if np.random.rand() < mutation_rate:
            individual["weights"][i] += np.random.randn(*individual["weights"][i].shape) * 0.1
            individual["biases"][i] += np.random.randn(*individual["biases"][i].shape) * 0.1
    return individual

def genetic_algorithm(X, y, layers, pop_size=20, generations=50, mutation_rate=0.1):
    """Optimize MLP weights using Genetic Algorithm."""
    population = initialize_population(pop_size, layers)
    for gen in range(generations):
        fitnesses = [fitness_function(ind, X, y) for ind in population]
        parents = select_parents(population, fitnesses, num_parents=pop_size//2)
        children = [crossover(parents[i], parents[i+1]) for i in range(0, len(parents)-1, 2)]
        children = [mutate(child, mutation_rate) for child in children]
        population = parents + children  # New generation
    best_individual = max(population, key=lambda ind: fitness_function(ind, X, y))
    return best_individual["weights"], best_individual["biases"]

# Example MLP with 2 input neurons, 3 hidden neurons, and 1 output neuron
X = np.array([[0.5, 0.2]])  # Example input
y = np.array([[1]])  # Example expected output

layers = [2, 3, 1]
weights, biases = genetic_algorithm(X, y, layers)

# Activation functions for each layer
activation_functions = [relu, sigmoid]

# Forward pass
output = forward_propagation(X, weights, biases, activation_functions)
print("Output:", output)
